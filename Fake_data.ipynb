{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpp-cleaned.txt\n",
      "protein-100k-cleaned.txt\n",
      "haskell-cleaned.txt\n",
      "fortran-cleaned.txt\n",
      "dna-100k-cleaned.txt\n",
      "java-cleaned.txt\n",
      "rand-seq.txt\n",
      "rand-distro-seq.txt\n",
      "cpp&92&0.092&1616&1.616&4927&4.926&1494&13.205&2856&25.245&3453&30.525\\\\\n",
      "dna-100k&5&0.005&20&0.021&72&0.076&1632&32.956&3261&98.788&1630&98.788\\\\\n",
      "fortran&83&0.083&1621&1.621&7173&7.173&2576&13.733&6989&37.261&9601&51.189\\\\\n",
      "haskell&117&0.117&2239&2.239&9120&9.118&3734&27.766&9103&67.695&11080&82.404\\\\\n",
      "java&89&0.089&1090&1.09&3046&3.046&985&9.079&1953&18.003&2811&25.915\\\\\n",
      "protein-100k&21&0.018&401&0.346&7819&6.803&584&19.61&1165&58.69&582&58.669\\\\\n",
      "rand-distro-seq.txt&53&0.055&1607&1.666&15283&15.843&10344&67.52&15014&98.015&15313&99.98\\\\\n",
      "rand-seq.txt&53&0.053&2808&2.809&72709&72.738&1812&99.342&1822&100.0&1820&100.0\\\\\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "datapath= '../data/fake'\n",
    "files= os.listdir(datapath)\n",
    "# print(files)\n",
    "\n",
    "char_dist={}\n",
    "word_dist={}\n",
    "\n",
    "word_dist_2={}\n",
    "file_features={}\n",
    "\n",
    "for file in files:\n",
    "    if file.endswith('.txt')==False:\n",
    "        continue\n",
    "    file_features[file]=[]\n",
    "    fp=open(datapath+'/'+file)\n",
    "    print(file)\n",
    "\n",
    "    if file not in char_dist:\n",
    "        char_dist[file]={}\n",
    "        char_dist[file]['uni']={}\n",
    "        char_dist[file]['bi']={}\n",
    "        char_dist[file]['tri']={}\n",
    "        char_dist[file]['bi-rev']={}\n",
    "        char_dist[file]['tri-rev']={}\n",
    "        char_dist[file]['neighbourhood']={}\n",
    "        \n",
    "    if file not in word_dist:\n",
    "        word_dist[file]={}\n",
    "        word_dist[file]['uni']={}\n",
    "        word_dist[file]['bi']={}\n",
    "        word_dist[file]['tri']={}\n",
    "        word_dist[file]['bi-rev']={}\n",
    "        word_dist[file]['tri-rev']={}\n",
    "        word_dist[file]['neighbourhood']={}\n",
    "    \n",
    "    if file not in word_dist_2:\n",
    "        word_dist_2[file]={}\n",
    "        word_dist_2[file]['uni']={}\n",
    "        word_dist_2[file]['bi']={}\n",
    "        word_dist_2[file]['tri']={}\n",
    "        word_dist_2[file]['bi-rev']={}\n",
    "        word_dist_2[file]['tri-rev']={}\n",
    "        word_dist_2[file]['neighbourhood']={}\n",
    "        \n",
    "    doc= fp.read()\n",
    "    sents= doc.split('\\n')\n",
    "    for sent in sents:\n",
    "        doc= re.sub('\\s+', ' ', sent).strip()        \n",
    "#         if char_rem_flag:\n",
    "#             doc= re.sub(\"[a-zA-Z0-9!@#$%^&*\\(\\)\\_\\+-=:;\\\\'>.<.,?/|]+\", \"\", doc)\n",
    "#         else:\n",
    "#             doc= re.sub(\"[0-9!@#$%^&*\\(\\)\\_\\+-=:;\\\\'>.<.,?/|]+\", \"\", doc)\n",
    "        doc_len= len(doc)\n",
    "        for i in range(doc_len):\n",
    "            tok= doc[i]\n",
    "            if tok not in char_dist[file]['uni']:\n",
    "                char_dist[file]['uni'][tok]=0\n",
    "            char_dist[file]['uni'][tok]+=1\n",
    "         \n",
    "        for i in range(doc_len-1):\n",
    "            tok= doc[i]+ doc[i+1]\n",
    "            if tok not in char_dist[file]['bi']:\n",
    "                char_dist[file]['bi'][tok]=0\n",
    "            char_dist[file]['bi'][tok]+=1\n",
    "        \n",
    "        for i in range(doc_len-2):\n",
    "            tok= doc[i]+ doc[i+1]+ doc[i+2]\n",
    "            if tok not in char_dist[file]['tri']:\n",
    "                char_dist[file]['tri'][tok]=0\n",
    "            char_dist[file]['tri'][tok]+=1\n",
    "        \n",
    "        for i in range(doc_len-1,0,-1):\n",
    "            tok= doc[i]+doc[i-1]\n",
    "            if tok not in char_dist[file]['bi-rev']:\n",
    "                char_dist[file]['bi-rev'][tok]=0\n",
    "            char_dist[file]['bi-rev'][tok]+=1\n",
    "        \n",
    "        for i in range(doc_len-1,1,-1):\n",
    "            tok= doc[i]+doc[i-1]+doc[i-2]\n",
    "            if tok not in char_dist[file]['tri-rev']:\n",
    "                char_dist[file]['tri-rev'][tok]=0\n",
    "            char_dist[file]['tri-rev'][tok]+=1\n",
    "        \n",
    "        for i in range(2,doc_len-2):\n",
    "            tok= doc[i-2]+doc[i-1]+doc[i]+doc[i+1]+doc[i+2]\n",
    "            if tok not in char_dist[file]['neighbourhood']:\n",
    "                char_dist[file]['neighbourhood'][tok]=0\n",
    "            char_dist[file]['neighbourhood'][tok]+=1\n",
    "        \n",
    "        # using words here\n",
    "        \n",
    "        words=['<S>']\n",
    "        words.extend(doc.split())\n",
    "        words.append('</S>')\n",
    "        doc_len=len(words)\n",
    "        \n",
    "        for i in range(doc_len):\n",
    "            tok= words[i]\n",
    "            if tok not in word_dist_2[file]['uni']:\n",
    "                word_dist_2[file]['uni'][tok]=0\n",
    "            word_dist_2[file]['uni'][tok]+=1\n",
    "        \n",
    "        for i in range(doc_len-1):\n",
    "            tok= words[i]+'_'+words[i+1]\n",
    "            if tok not in word_dist_2[file]['bi']:\n",
    "                word_dist_2[file]['bi'][tok]=0\n",
    "            word_dist_2[file]['bi'][tok]+=1\n",
    "        \n",
    "        for i in range(doc_len-2):\n",
    "            tok= words[i]+'_'+words[i+1]+'_'+words[i+2]\n",
    "            if tok not in word_dist_2[file]['tri']:\n",
    "                word_dist_2[file]['tri'][tok]=0\n",
    "            word_dist_2[file]['tri'][tok]+=1\n",
    "        \n",
    "        \n",
    "        for i in range(doc_len):\n",
    "            tok= words[i]\n",
    "            if tok not in word_dist[file]['uni']:\n",
    "                word_dist[file]['uni'][tok]=0\n",
    "            word_dist[file]['uni'][tok]+=1\n",
    "         \n",
    "        for i in range(doc_len-1):\n",
    "            tok1=words[i]\n",
    "            tok2=words[i+1]\n",
    "            if tok1 not in word_dist[file]['bi']:\n",
    "                word_dist[file]['bi'][tok1]={}\n",
    "            if tok2 not in word_dist[file]['bi'][tok1]:\n",
    "                word_dist[file]['bi'][tok1][tok2]=0\n",
    "            word_dist[file]['bi'][tok1][tok2]+=1\n",
    "        \n",
    "        for i in range(doc_len-2):\n",
    "            tok1=words[i]\n",
    "            tok2=words[i+1]\n",
    "            tok3=words[i+2]\n",
    "            if tok1 not in word_dist[file]['tri']:\n",
    "                word_dist[file]['tri'][tok1]={}\n",
    "            if tok2 not in word_dist[file]['tri'][tok1]:\n",
    "                word_dist[file]['tri'][tok1][tok2]={}\n",
    "            if tok3 not in word_dist[file]['tri'][tok1][tok2]:\n",
    "                word_dist[file]['tri'][tok1][tok2][tok3]=0\n",
    "            word_dist[file]['tri'][tok1][tok2][tok3]+=1\n",
    "        \n",
    "        for i in range(2,doc_len-2):\n",
    "            tok1=words[i-1]\n",
    "            tok2=words[i-2]\n",
    "            tok=words[i]\n",
    "            tok4=words[i+1]\n",
    "            tok5=words[i+2]\n",
    "            \n",
    "            if tok1 not in word_dist[file]['neighbourhood']:\n",
    "                word_dist[file]['neighbourhood'][tok1]={}\n",
    "            if tok2 not in word_dist[file]['neighbourhood'][tok1]:\n",
    "                word_dist[file]['neighbourhood'][tok1][tok2]={}\n",
    "            if tok4 not in word_dist[file]['neighbourhood'][tok1][tok2]:\n",
    "                word_dist[file]['neighbourhood'][tok1][tok2][tok4]={}\n",
    "            if tok5 not in word_dist[file]['neighbourhood'][tok1][tok2][tok4]:\n",
    "                word_dist[file]['neighbourhood'][tok1][tok2][tok4][tok5]={}\n",
    "            if tok3 not in word_dist[file]['neighbourhood'][tok1][tok2][tok4][tok5]:\n",
    "                word_dist[file]['neighbourhood'][tok1][tok2][tok4][tok5][tok3]=0\n",
    "            word_dist[file]['neighbourhood'][tok1][tok2][tok4][tok5][tok3]+=1\n",
    "    \n",
    "    \n",
    "for file in sorted(char_dist):\n",
    "    f_name=file.replace('-cleaned.txt','')\n",
    "    c_uni=str(len(char_dist[file]['uni']))\n",
    "    c_bi=str(len(char_dist[file]['bi']))\n",
    "    c_tri=str(len(char_dist[file]['tri']))\n",
    "    c_uni_tot=str(round(100*int(c_uni)/sum([char_dist[file]['uni'][i] for i in char_dist[file]['uni']]),3))\n",
    "    c_bi_tot=str(round(100*int(c_bi)/sum([char_dist[file]['bi'][i] for i in char_dist[file]['bi']]),3))\n",
    "    c_tri_tot=str(round(100*int(c_tri)/sum([char_dist[file]['tri'][i] for i in char_dist[file]['tri']]),3))\n",
    "    \n",
    "    w_uni=str(len(word_dist_2[file]['uni']))\n",
    "    w_bi=str(len(word_dist_2[file]['bi']))\n",
    "    w_tri=str(len(word_dist_2[file]['tri']))\n",
    "    w_uni_tot=str(round(100*int(w_uni)/sum([word_dist_2[file]['uni'][i] for i in word_dist_2[file]['uni']]),3))\n",
    "    w_bi_tot=str(round(100*int(w_bi)/sum([word_dist_2[file]['bi'][i] for i in word_dist_2[file]['bi']]),3))\n",
    "    w_tri_tot=str(round(100*int(w_tri)/sum([word_dist_2[file]['tri'][i] for i in word_dist_2[file]['tri']]),3))\n",
    "    \n",
    "    file_features[file].extend([c_uni_tot,c_bi_tot,c_tri_tot,w_uni_tot,w_bi_tot,w_tri_tot])\n",
    "    \n",
    "    \n",
    "#     w_uni=str(len(word_dist[file]['uni']))\n",
    "#     w_uni_tot=str(round(100*int(w_uni)/sum([word_dist[file]['uni'][i] for i in word_dist[file]['uni']]),3))\n",
    "    \n",
    "#     w_bi=str(sum([1 for i in word_dist[file]['bi'] for j in word_dist[file]['bi'][i]]))\n",
    "#     w_bi_tot= str(round(100*int(w_bi)/sum([word_dist[file]['bi'][i][j] for i in word_dist[file]['bi'] for j in word_dist[file]['bi'][i]]),3))\n",
    "    \n",
    "#     w_tri=str(sum([1 for i in word_dist[file]['tri'] for j in word_dist[file]['tri'][i] for k in word_dist[file]['tri'][i][j]]))\n",
    "#     w_tri_tot= str(round(100*int(w_tri)/sum([word_dist[file]['tri'][i][j][k] for i in word_dist[file]['tri'] for j in word_dist[file]['tri'][i] for k in word_dist[file]['tri'][i][j]]),3))\n",
    "    \n",
    "    print(f_name+'&'+c_uni+'&'+c_uni_tot+'&'+c_bi+'&'+c_bi_tot+'&'+c_tri+'&'+c_tri_tot+'&'\\\n",
    "         +w_uni+'&'+w_uni_tot+'&'+w_bi+'&'+w_bi_tot+'&'+w_tri+'&'+w_tri_tot+'\\\\\\\\')\n",
    "    \n",
    "    \n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename\tuni-entropy\tbi-entropy\ttri-entropy\tcond-bi-entropy\tcond-tri-entropy\tuni-w-entropy\tbi-w-entropy\ttri-w-entropy\tcond-bi-w-entropy\tcond-tri-w-entropy\n",
      "cpp&8.62&10.344&3.354&2.001&1.56&0.553&10.18&10.897&5.266&8.343&\\\\\n",
      "dna-100k&3.955&11.662&1.949&4.748&1.94&-1.0&5.895&10.662&2.007&5.137&\\\\\n",
      "fortran&8.864&11.72&3.742&2.939&2.163&0.911&11.027&12.63&5.122&8.781&\\\\\n",
      "haskell&8.982&12.692&3.895&2.776&2.466&0.531&11.448&13.223&5.088&9.916&\\\\\n",
      "java&8.701&9.878&3.412&2.125&1.554&0.809&10.255&10.687&5.289&7.754&\\\\\n",
      "protein-100k&8.334&10.129&4.151&3.982&4.046&-1.001&12.381&9.128&4.184&4.625&\\\\\n",
      "rand-distro-seq.txt&8.502&13.852&4.222&1.77&4.001&0.05&12.503&13.902&4.28&12.083&\\\\\n",
      "rand-seq.txt&11.435&10.831&5.708&0.012&4.583&-0.002&16.018&10.83&5.728&10.818&\\\\\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import operator\n",
    "\n",
    "def entropy(given_dict):\n",
    "    H=0\n",
    "    #sorted_dict = sorted(given_dict.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    count=0\n",
    "    per_elem=0\n",
    "            \n",
    "    for elem in given_dict:\n",
    "        count+=given_dict[elem]\n",
    "        per_elem+= math.log(given_dict[elem])*given_dict[elem]        \n",
    "    return (math.log(count)- per_elem/count)/(math.log(2))\n",
    "\n",
    "def cond_entropy(dict1, dict2):\n",
    "    N_dict1=sum([dict1[elem] for elem in dict1])\n",
    "    N_dict2=sum([dict2[elem] for elem in dict2])\n",
    "    prob_dict1={}\n",
    "    prob_dict2={}\n",
    "    \n",
    "    for elem in dict1:\n",
    "        prob_dict1[elem]= dict1[elem]/N_dict1\n",
    "    for elem in dict2:\n",
    "        prob_dict2[elem]= dict2[elem]/N_dict2\n",
    "\n",
    "    cond_H=0\n",
    "    for elem in prob_dict2:\n",
    "        cond_H+= - (prob_dict2[elem])*math.log(prob_dict2[elem]/prob_dict1[elem[:-1]])/math.log(2)\n",
    "    \n",
    "    return cond_H\n",
    "\n",
    "def cond_entropy_2(dict1, dict2):\n",
    "    N_dict1=sum([dict1[elem] for elem in dict1])\n",
    "    N_dict2=sum([dict2[elem1][elem2] for elem1 in dict2 for elem2 in dict2[elem1]])\n",
    "    prob_dict1={}\n",
    "    prob_dict2={}\n",
    "    \n",
    "    for elem in dict1:\n",
    "        prob_dict1[elem]= dict1[elem]/N_dict1\n",
    "    for elem1 in dict2:\n",
    "        if elem1 not in prob_dict2:\n",
    "            prob_dict2[elem1]={}\n",
    "        for elem2 in dict2[elem1]:\n",
    "            prob_dict2[elem1][elem2]= dict2[elem1][elem2]/N_dict2\n",
    "        \n",
    "\n",
    "    cond_H=0\n",
    "    for elem1 in prob_dict2:\n",
    "        for elem2 in prob_dict2[elem1]:\n",
    "            cond_H+= - (prob_dict2[elem1][elem2])*math.log(prob_dict2[elem1][elem2]/prob_dict1[elem1])/math.log(2)\n",
    "                                                                                               \n",
    "    return cond_H\n",
    "\n",
    "def cond_entropy_3(dict1, dict2):\n",
    "    N_dict1=sum([dict1[elem1][elem2] for elem1 in dict1 for elem2 in dict1[elem1]])\n",
    "    N_dict2=sum([dict2[elem1][elem2][elem3] for elem1 in dict2 for elem2 in dict2[elem1] for elem3 in dict2[elem1][elem2]])\n",
    "    prob_dict1={}\n",
    "    prob_dict2={}\n",
    "    \n",
    "    for elem1 in dict1:\n",
    "        if elem1 not in prob_dict1:\n",
    "            prob_dict1[elem1]={}\n",
    "        for elem2 in dict1[elem1]:\n",
    "            prob_dict1[elem1][elem2]= dict1[elem1][elem2]/N_dict1\n",
    "            \n",
    "    for elem1 in dict2:\n",
    "        if elem1 not in prob_dict2:\n",
    "            prob_dict2[elem1]={}\n",
    "        for elem2 in dict2[elem1]:\n",
    "            if elem2 not in prob_dict2[elem1]:\n",
    "                prob_dict2[elem1][elem2]={}\n",
    "            for elem3 in dict2[elem1][elem2]:\n",
    "                prob_dict2[elem1][elem2][elem3]= dict2[elem1][elem2][elem3]/N_dict2\n",
    "        \n",
    "\n",
    "    cond_H=0\n",
    "    for elem1 in prob_dict2:\n",
    "        for elem2 in prob_dict2[elem1]:\n",
    "            for elem3 in prob_dict2[elem1][elem2]:\n",
    "                cond_H+= - (prob_dict2[elem1][elem2][elem3])*math.log(prob_dict2[elem1][elem2][elem3]/prob_dict1[elem1][elem2])/math.log(2)\n",
    "                                                                                               \n",
    "    return cond_H\n",
    "\n",
    "test_dict={'a':10, 'b':10,'c':10, 'd':10}\n",
    "\n",
    "stats_dict={}\n",
    "features=['uni-entropy','bi-entropy','bi-rev-entropy','tri-entropy','tri-rev-entropy','cond-bi-entropy','cond-tri-entropy',\\\n",
    "          'neighbourhood-entropy','cond-bi-rev-entropy','cond-tri-rev-entropy','uni-w-entropy','bi-w-entropy','tri-w-entropy',\\\n",
    "         'cond-bi-w-entropy','cond-tri-w-entropy','neighbourhood-w-entropy']\n",
    "\n",
    "\n",
    "for file in char_dist:\n",
    "    stats_dict[file]={}\n",
    "    stats_dict[file]['uni-entropy']=entropy(char_dist[file]['uni'])\n",
    "    stats_dict[file]['bi-entropy']=entropy(char_dist[file]['bi'])\n",
    "    stats_dict[file]['tri-entropy']=entropy(char_dist[file]['tri'])\n",
    "    stats_dict[file]['uni-w-entropy']= entropy(word_dist_2[file]['uni'])\n",
    "    stats_dict[file]['bi-w-entropy']= entropy(word_dist_2[file]['bi'])\n",
    "    stats_dict[file]['tri-w-entropy']= entropy(word_dist_2[file]['tri'])\n",
    "    \n",
    "    stats_dict[file]['cond-bi-entropy']= cond_entropy(char_dist[file]['uni'], char_dist[file]['bi'])\n",
    "    stats_dict[file]['cond-tri-entropy']= cond_entropy(char_dist[file]['bi'], char_dist[file]['tri'])\n",
    "    stats_dict[file]['cond-bi-w-entropy']= cond_entropy_2(word_dist[file]['uni'], word_dist[file]['bi'])\n",
    "    stats_dict[file]['cond-tri-w-entropy']= cond_entropy_3(word_dist[file]['bi'], word_dist[file]['tri'])\n",
    "    \n",
    "    \n",
    "    \n",
    "#     stats_dict[file]['bi-rev-entropy']=entropy(char_dist[file]['bi-rev'])\n",
    "#     stats_dict[file]['tri-rev-entropy']=entropy(char_dist[file]['tri-rev'])\n",
    "#     stats_dict[file]['neighbourhood-entropy']=entropy(char_dist[file]['neighbourhood'])\n",
    "#     stats_dict[file]['cond-bi-entropy']= cond_entropy(char_dist[file]['uni'], char_dist[file]['bi'])\n",
    "#     stats_dict[file]['cond-tri-entropy']= cond_entropy(char_dist[file]['bi'], char_dist[file]['tri'])\n",
    "#     stats_dict[file]['cond-bi-rev-entropy']= cond_entropy(char_dist[file]['uni'], char_dist[file]['bi-rev'])\n",
    "#     stats_dict[file]['cond-tri-rev-entropy']= cond_entropy(char_dist[file]['bi-rev'], char_dist[file]['tri-rev'])\n",
    "\n",
    "features=['uni-entropy','bi-entropy','tri-entropy','cond-bi-entropy', 'cond-tri-entropy',\\\n",
    "          'uni-w-entropy','bi-w-entropy','tri-w-entropy','cond-bi-w-entropy','cond-tri-w-entropy']\n",
    "print(\"Filename\\t\"+\"\\t\".join(features))\n",
    "\n",
    "\n",
    "for file in sorted(char_dist):\n",
    "    f_name= file.replace('-cleaned.txt','')\n",
    "    str_file= f_name+'&'\n",
    "    for feature in sorted(features):\n",
    "        str_file+= str(round(stats_dict[file][feature],3))+'&'\n",
    "        file_features[file].append(stats_dict[file][feature])\n",
    "    print(str_file+'\\\\\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cpp-cleaned.txt': ['0.092', '1.616', '4.926', '13.205', '25.245', '30.525', 8.620079052459579, 10.343826224433217, 3.3539547459471173, 2.001307079272228, 1.5598796989834196, 0.553126190805034, 10.179975212973194, 10.896676436833749, 5.266130321186844, 8.342971973719473], 'protein-100k-cleaned.txt': ['0.018', '0.346', '6.803', '19.61', '58.69', '58.669', 8.333996888013766, 10.128726818478688, 4.150527340878508, 3.9820296908162662, 4.046297968856993, -1.0007269816434379, 12.381300959647966, 9.127583407161067, 4.183789125382607, 4.625446804078556], 'haskell-cleaned.txt': ['0.117', '2.239', '9.118', '27.766', '67.695', '82.404', 8.982043142500167, 12.69206143997818, 3.8945401535635003, 2.776349132042055, 2.4656331052056273, 0.5314954317215937, 11.447651672968957, 13.223480794406433, 5.087517295984849, 9.915994832602019], 'fortran-cleaned.txt': ['0.083', '1.621', '7.173', '13.733', '37.261', '51.189', 8.863872039520317, 11.71968932662056, 3.741802537856948, 2.939202231055471, 2.163254261706609, 0.9107537735305307, 11.027130391949626, 12.630311118270411, 5.122076247578447, 8.780775758071856], 'dna-100k-cleaned.txt': ['0.005', '0.021', '0.076', '32.956', '98.788', '98.788', 3.9553382900296143, 11.662325850969081, 1.9487945435695633, 4.747892314013221, 1.9395604077333, -1.0004371140893797, 5.894633911704974, 10.661880748524496, 2.0065995108264754, 5.137487327803134], 'java-cleaned.txt': ['0.089', '1.09', '3.046', '9.079', '18.003', '25.915', 8.701063417287237, 9.878328428326306, 3.4119508937209693, 2.125167876597256, 1.5544053928014647, 0.8087407449573538, 10.255451818733045, 10.686744031490091, 5.289119146965033, 7.753681531902107], 'rand-seq.txt': ['0.053', '2.809', '72.738', '99.342', '100.0', '100.0', 11.435353062703454, 10.831307243802051, 5.707799406848028, 0.012418203843854558, 4.582774515430968, -0.0015845087159921045, 16.018128695357273, 10.82972273508606, 5.727553752224563, 10.817807900675527], 'rand-distro-seq.txt': ['0.055', '1.666', '15.843', '67.52', '98.015', '99.98', 8.502260800245516, 13.852205076676062, 4.221759834259966, 1.7696549104433814, 4.000685522320485, 0.05016174611174519, 12.502956771679393, 13.90236019765768, 4.280490265116245, 12.082657291363553]}\n"
     ]
    }
   ],
   "source": [
    "print(file_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
